{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "#from diffusers import DDIMScheduler,DDPMScheduler\n",
    "import os\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use CUDA\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class catDataset(Dataset):\n",
    "    def __init__(self, img_dir, second_img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = []\n",
    "\n",
    "        for filename in os.listdir(img_dir):\n",
    "            file_path = os.path.join(img_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                self.img_names.append(file_path)\n",
    "        \n",
    "        for filename in os.listdir(second_img_dir):\n",
    "            file_path = os.path.join(second_img_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                self.img_names.append(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.img_names[idx])\n",
    "        image = image.to(torch.float32)/255.0\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=catDataset(\"./cats\",\"./mycat_64x64\")\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMScheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps: int=1000):\n",
    "        super().__init__()\n",
    "        self.beta = torch.linspace(1e-4, 0.02, num_time_steps, requires_grad=False)\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(alpha, dim=0).requires_grad_(False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.beta[t], self.alpha[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1000\n",
    "sch = DDPMScheduler(total_timesteps)\n",
    "#sch.set_timesteps(50)\n",
    "# Inference and training are totally seperate\n",
    "# train always is t -> t-1 \n",
    "#sch.step(img1,25,)\n",
    "# Use gaussian noise \n",
    "# img, gaussian, timestep\n",
    "# sch.add_noise(img1,torch.randn_like(img1),torch.tensor(999))\n",
    "# plt.imshow can work with floats just complains a bit\n",
    "# .permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        # Initialize EMA parameters with the same values as model parameters\n",
    "        self.ema_params = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                self.ema_params[name] = self.decay * self.ema_params[name] + (1 - self.decay) * param\n",
    "\n",
    "    def apply(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.data = self.ema_params[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.data = self.ema_params[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, ema):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X) in enumerate(dataloader):\n",
    "        x_0 = X.to(device)\n",
    "        curr_bs = X.shape[0]\n",
    "        t = torch.randint(0,total_timesteps,(curr_bs,))\n",
    "        noise = torch.randn_like(x_0,requires_grad=False)\n",
    "        a = sch.alpha[t].view(curr_bs,1,1,1).to(device)\n",
    "        x = (torch.sqrt(a)*x_0) + (torch.sqrt(1-a)*noise)\n",
    "\n",
    "        pred = model(x,t[0])\n",
    "        loss = loss_fn(noise,pred)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            #print(torch.mps.current_allocated_memory()/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 35455555 parameters\n",
      "Epoch 0\n",
      "loss: 1.253199  [   64/29854]\n",
      "loss: 1.000623  [ 6464/29854]\n",
      "loss: 0.998806  [12864/29854]\n",
      "loss: 0.998822  [19264/29854]\n",
      "loss: 1.000440  [25664/29854]\n",
      "Epoch 1\n",
      "loss: 0.998288  [   64/29854]\n",
      "loss: 0.995701  [ 6464/29854]\n",
      "loss: 0.999130  [12864/29854]\n",
      "loss: 1.002276  [19264/29854]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlosf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 5\u001b[0m     x_0 \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     curr_bs \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,total_timesteps,(curr_bs,))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losf = nn.MSELoss()\n",
    "model = UNet(3,3).to(device)\n",
    "ema = EMA(model,decay=0.999)\n",
    "print(f\"Model has {get_model_size(model)} parameters\")\n",
    "#print(torch.mps.current_allocated_memory()/1e9)\n",
    "epochs = 5\n",
    "opt = torch.optim.AdamW(model.parameters(),2e-5)\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    train_loop(train_dataloader,model,losf,opt,ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1692, -0.9671,  1.1251,  ..., -1.0551, -0.7499, -1.6292],\n",
      "          [ 0.6439,  0.2862,  0.5339,  ...,  1.0847,  0.1038,  0.2279],\n",
      "          [ 0.6312,  0.3369,  0.4150,  ...,  1.7995,  2.3065, -0.8919],\n",
      "          ...,\n",
      "          [-0.4395,  0.4910,  0.1439,  ..., -0.2693, -1.0838,  0.1068],\n",
      "          [-1.4824,  0.3024,  0.9480,  ...,  0.7726,  0.8500, -0.2259],\n",
      "          [-0.1618,  1.5281, -0.1929,  ..., -1.0863,  1.5868,  1.3455]],\n",
      "\n",
      "         [[-0.0499,  0.7305,  0.3193,  ..., -0.0198, -1.0959,  2.1072],\n",
      "          [ 0.5687, -0.1067,  0.8327,  ..., -0.2662,  0.1121,  0.2600],\n",
      "          [ 2.1679, -1.5367, -0.5600,  ...,  0.1887, -0.6503, -0.5361],\n",
      "          ...,\n",
      "          [ 0.6410, -0.7314,  0.5745,  ...,  0.6057,  0.7800, -0.5803],\n",
      "          [ 0.7338,  0.5559,  1.6077,  ...,  1.3705, -1.3864, -1.5120],\n",
      "          [-1.3322, -0.7367, -0.5126,  ...,  0.0476, -1.2847, -1.5150]],\n",
      "\n",
      "         [[ 1.6886, -0.2865, -0.1043,  ...,  2.0151, -0.2910,  0.1858],\n",
      "          [ 0.0808,  1.0636, -0.3012,  ..., -0.5613,  1.1491, -1.8417],\n",
      "          [-0.0500, -0.4614, -0.9136,  ..., -1.3803,  1.1662,  0.6014],\n",
      "          ...,\n",
      "          [-0.5244,  0.9151, -0.4404,  ..., -1.4957,  0.8887, -1.0679],\n",
      "          [-0.9540,  0.2943, -0.3939,  ..., -0.2498,  1.9953, -1.5487],\n",
      "          [-0.0250,  0.0681,  0.7905,  ...,  0.2381,  2.2094,  0.4801]]]],\n",
      "       device='mps:0')\n",
      "tensor([[[[4.0237e-12, 1.3552e-20, 1.6709e-29,  ..., 6.9627e-27,\n",
      "           1.4784e-19, 2.2822e-09],\n",
      "          [5.1821e-17, 1.4211e-32, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           6.1684e-37, 2.6524e-18],\n",
      "          [6.2281e-22, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 3.9133e-26],\n",
      "          ...,\n",
      "          [2.6167e-18, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.1382e-25],\n",
      "          [8.9560e-12, 3.2147e-28, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           9.8475e-34, 1.8312e-19],\n",
      "          [4.9851e-05, 1.3841e-14, 1.0005e-21,  ..., 6.7881e-23,\n",
      "           2.5420e-19, 2.0560e-10]],\n",
      "\n",
      "         [[1.6543e-12, 5.4322e-23, 3.7833e-33,  ..., 2.9329e-35,\n",
      "           8.5204e-26, 1.0193e-16],\n",
      "          [1.0622e-21, 2.9461e-36, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 5.2867e-26],\n",
      "          [4.1845e-31, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 4.3535e-34],\n",
      "          ...,\n",
      "          [4.5532e-31, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3731e-31],\n",
      "          [1.0367e-22, 5.3006e-36, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           4.2152e-38, 2.0906e-22],\n",
      "          [1.8547e-14, 2.3395e-19, 1.1412e-28,  ..., 1.4961e-25,\n",
      "           5.5907e-19, 1.0564e-10]],\n",
      "\n",
      "         [[3.4030e-13, 7.1836e-22, 2.1969e-30,  ..., 4.3586e-26,\n",
      "           2.9812e-17, 4.0544e-09],\n",
      "          [7.5088e-23, 1.0495e-35, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           2.9878e-27, 2.9049e-13],\n",
      "          [6.7449e-29, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.4790e-38, 1.0196e-21],\n",
      "          ...,\n",
      "          [1.3586e-26, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           6.1768e-38, 1.7957e-19],\n",
      "          [5.2414e-20, 6.2165e-32, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.5409e-27, 1.5804e-13],\n",
      "          [1.2327e-10, 1.2925e-17, 8.5081e-23,  ..., 1.3630e-22,\n",
      "           5.1685e-15, 1.5079e-07]]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "with torch.no_grad():\n",
    "    # Initialize z\n",
    "    z = torch.randn_like(ds[0]).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Loop through timesteps in reverse order\n",
    "    for t in reversed(range(1, total_timesteps)):\n",
    "        print(z)\n",
    "        print(model(z,t))\n",
    "        break\n",
    "        x = i.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "        plt.imshow(x)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.564282368"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.current_allocated_memory()/1e9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
